{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Game time - Continued!\n",
    "\n",
    "We introduced you to this game on Monday. Today we're going to go all in with the game. Before class on Friday, make sure that you have a classifier for each disease (D1 and D2) that you feel performs the best of all of the classifiers that you've constructed and that you can estimate the accuracy for.\n",
    "\n",
    "To help you along, we've collapsed the datasets that you're familiar with down to two files. In addition to the 3000 examples of each that you've already seen (2000 in S1 and 1000 in S2) - we've added another 1000 for a total of 4000 individuals for each disease! They're formatted just like the samples that you've already worked with, but they're all in one dataset (D1.csv and D2.csv).\n",
    "\n",
    "You should start with the code from last time, but you'll need to adapt it to this new setting! This is particularly important because now you only have one dataset, instead of separate training and testing datasets.\n",
    "\n",
    "In the interests of recording your research steps, remember that whatever you change should be recorded and noted in the iPython notebook. We provide an example first move below. In every case, please label the move number, the goal (what you hope to implement), the rationale (why you've chosen to implement that, or make that move as a result of the prior move), and an expected accuracy which you fill out after you build and run your code.\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Instead of using a separate training and testing dataset, let's try out cross validation! We're going to try out some real world troubleshooting with these methods. We know what we want (cross validation!) but not how to get there. Googling for cross validation and scikit-learn leads us to this page: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Read through the documentation, particularly 3.1.1. Their features are iris.data and their labels are iris.target. You'll need to adapt these to fit your datsets.\n",
    "\n",
    "### Saving Your Final Classifier!\n",
    "\n",
    "Once you have your final classifiers, we'll provide some code that will help you to save your final model by pickling them. 'pickle' is a python feature that lets you save python objects (like your classifier) for later re-use. Scikit learn provides a tool for this. If you have been using the `classifier` variable (as we did in the example), this will save your classifier to a file. We'll be loading them back in class on Friday and assigning them to a few final test datasets! This will let us see whose model reigns supreme!\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(classifier, 'filename.pkl') \n",
    "\n",
    "Here, you'll have built two classifiers. One for D1 and one for D2. To be eligible for the competition you will need to follow this file naming scheme `<Disease>-<ExpectedAccuracy>.pkl`\n",
    "\n",
    "If I had a classifier for Disease 1 that I thought would have an accuracy of 0.6, I'd use the following lines of code to save it:\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(classifier, 'D1-0.6.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "ML_4.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
